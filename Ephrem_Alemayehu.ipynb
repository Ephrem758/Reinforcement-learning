{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Ephrem Alemayehu\n",
    "# ID: UGR/4365/12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid World Environment\n",
    "class GridWorld:\n",
    "    def __init__(self, n, m, start, goal, obstacles):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles\n",
    "        self.grid = np.zeros((n, m))\n",
    "        for obstacle in obstacles:\n",
    "            self.grid[obstacle] = -1  # Mark obstacles with -1\n",
    "        self.grid[goal] = 10  # Mark goal with 10\n",
    "        \n",
    "    def is_valid(self, state):\n",
    "        x, y = state\n",
    "        if 0 <= x < self.n and 0 <= y < self.m and self.grid[x, y] != -1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        if action == \"up\":\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == \"down\":\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == \"left\":\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        elif action == \"right\":\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        else:\n",
    "            next_state = state\n",
    "        \n",
    "        if self.is_valid(next_state):\n",
    "            return next_state\n",
    "        else:\n",
    "            return state\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        if state == self.goal:\n",
    "            return 10\n",
    "        elif self.grid[state] == -1:\n",
    "            return -10\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "# Example setup for GridWorld\n",
    "n, m = 4, 4\n",
    "start = (0, 0)\n",
    "goal = (3, 3)\n",
    "obstacles = [(1, 1), (1, 2), (2, 1)]\n",
    "\n",
    "grid_world = GridWorld(n, m, start, goal, obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (Value Iteration):\n",
      " [[ 1.8098  3.122   4.58    6.2   ]\n",
      " [ 3.122   0.      0.      8.    ]\n",
      " [ 4.58    0.      8.     10.    ]\n",
      " [ 6.2     8.     10.      0.    ]]\n",
      "Policy (Value Iteration):\n",
      " [['d' 'r' 'r' 'd']\n",
      " ['d' '' '' 'd']\n",
      " ['d' '' 'd' 'd']\n",
      " ['r' 'r' 'r' '']]\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "def value_iteration(grid_world, gamma=0.9, theta=1e-6):\n",
    "    V = np.zeros((grid_world.n, grid_world.m))\n",
    "    policy = np.zeros((grid_world.n, grid_world.m), dtype=str)\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(grid_world.n):\n",
    "            for j in range(grid_world.m):\n",
    "                state = (i, j)\n",
    "                if state == grid_world.goal or grid_world.grid[state] == -1:\n",
    "                    continue\n",
    "                v = V[state]\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "                for action in actions:\n",
    "                    next_state = grid_world.get_next_state(state, action)\n",
    "                    reward = grid_world.get_reward(next_state)\n",
    "                    value = reward + gamma * V[next_state]\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                        best_action = action\n",
    "                V[state] = max_value\n",
    "                policy[state] = best_action\n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "V, policy = value_iteration(grid_world)\n",
    "print(\"Value Function (Value Iteration):\\n\", V)\n",
    "print(\"Policy (Value Iteration):\\n\", policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "def policy_evaluation(policy, grid_world, gamma=0.9, theta=1e-6):\n",
    "    V = np.zeros((grid_world.n, grid_world.m))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(grid_world.n):\n",
    "            for j in range(grid_world.m):\n",
    "                state = (i, j)\n",
    "                if state == grid_world.goal or grid_world.grid[state] == -1:\n",
    "                    continue\n",
    "                v = V[state]\n",
    "                action = policy[state]\n",
    "                next_state = grid_world.get_next_state(state, action)\n",
    "                reward = grid_world.get_reward(next_state)\n",
    "                V[state] = reward + gamma * V[next_state]\n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (Policy Iteration):\n",
      " [[-9.99999179 -9.99999179 -9.99999179 -9.99999179]\n",
      " [-9.99999179  0.          0.         -9.99999179]\n",
      " [-9.99999179  0.         -9.99999179 -9.99999179]\n",
      " [-9.99999179 -9.99999179 -9.99999179  0.        ]]\n",
      "Policy (Policy Iteration):\n",
      " [['u' 'u' 'u' 'u']\n",
      " ['u' '' '' 'u']\n",
      " ['u' '' 'u' 'd']\n",
      " ['u' 'u' 'r' '']]\n"
     ]
    }
   ],
   "source": [
    "def policy_improvement(V, grid_world, gamma=0.9):\n",
    "    policy = np.zeros((grid_world.n, grid_world.m), dtype=str)\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    for i in range(grid_world.n):\n",
    "        for j in range(grid_world.m):\n",
    "            state = (i, j)\n",
    "            if state == grid_world.goal or grid_world.grid[state] == -1:\n",
    "                continue\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "            for action in actions:\n",
    "                next_state = grid_world.get_next_state(state, action)\n",
    "                reward = grid_world.get_reward(next_state)\n",
    "                value = reward + gamma * V[next_state]\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(grid_world, gamma=0.9):\n",
    "    policy = np.random.choice([\"up\", \"down\", \"left\", \"right\"], size=(grid_world.n, grid_world.m))\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, grid_world, gamma)\n",
    "        new_policy = policy_improvement(V, grid_world, gamma)\n",
    "        if (new_policy == policy).all():\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return V, policy\n",
    "\n",
    "V_policy, policy_policy = policy_iteration(grid_world)\n",
    "print(\"Value Function (Policy Iteration):\\n\", V_policy)\n",
    "print(\"Policy (Policy Iteration):\\n\", policy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Values (Q-Learning):\n",
      " [[[ 0.20231665 -0.86826708  0.41113643  1.8098    ]\n",
      "  [ 1.50965301  1.6531591   0.46231559  3.122     ]\n",
      "  [ 2.82027661  2.88370545  1.52829271  4.58      ]\n",
      "  [ 3.55299774  6.2         2.89960885  3.49792067]]\n",
      "\n",
      " [[-2.02835574  1.19628643 -1.89610968 -1.80533789]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 4.49244171  8.          6.06083156  5.85871876]]\n",
      "\n",
      " [[-1.08599338  3.34259988 -1.30793839 -1.20479159]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.1         0.69521912 -0.1         7.65943287]\n",
      "  [ 5.69713239 10.          4.9361339   7.70025496]]\n",
      "\n",
      " [[-0.54165715 -0.58519851  0.29891246  5.62826773]\n",
      "  [ 0.95510404 -0.199       0.19996868  7.80594192]\n",
      "  [ 0.14574014 -0.1        -0.109       9.96956747]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "Policy (Q-Learning):\n",
      " [['r' 'r' 'r' 'd']\n",
      " ['d' 'u' 'u' 'd']\n",
      " ['d' 'u' 'r' 'd']\n",
      " ['r' 'r' 'r' 'u']]\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning\n",
    "def q_learning(grid_world, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
    "    Q = np.zeros((grid_world.n, grid_world.m, 4))\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    action_indices = {a: i for i, a in enumerate(actions)}\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = grid_world.start\n",
    "        while state != grid_world.goal:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(actions)\n",
    "            else:\n",
    "                action = actions[np.argmax(Q[state[0], state[1]])]\n",
    "            next_state = grid_world.get_next_state(state, action)\n",
    "            reward = grid_world.get_reward(next_state)\n",
    "            best_next_action = np.argmax(Q[next_state[0], next_state[1]])\n",
    "            td_target = reward + gamma * Q[next_state[0], next_state[1], best_next_action]\n",
    "            td_error = td_target - Q[state[0], state[1], action_indices[action]]\n",
    "            Q[state[0], state[1], action_indices[action]] += alpha * td_error\n",
    "            state = next_state\n",
    "\n",
    "    policy = np.zeros((grid_world.n, grid_world.m), dtype=str)\n",
    "    for i in range(grid_world.n):\n",
    "        for j in range(grid_world.m):\n",
    "            best_action_idx = np.argmax(Q[i, j])\n",
    "            policy[i, j] = actions[best_action_idx]\n",
    "    return Q, policy\n",
    "\n",
    "Q, policy_q_learning = q_learning(grid_world)\n",
    "print(\"Q-Values (Q-Learning):\\n\", Q)\n",
    "print(\"Policy (Q-Learning):\\n\", policy_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Values (UCB):\n",
      " [ 2.18530386  2.23517113  4.6713612   3.49230406  3.9093488   4.8336039\n",
      "  7.04370862  7.89710713  9.0669796  10.05967864]\n",
      "Total Reward (UCB):\n",
      " 9966.208897800314\n"
     ]
    }
   ],
   "source": [
    "# Multi-Armed Bandit Environment\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, k, reward_distributions):\n",
    "        self.k = k\n",
    "        self.reward_distributions = reward_distributions\n",
    "        self.q_true = [np.mean(dist) for dist in reward_distributions]\n",
    "        \n",
    "    def pull(self, arm):\n",
    "        return np.random.choice(self.reward_distributions[arm])\n",
    "\n",
    "# Example setup for MultiArmedBandit\n",
    "k = 10\n",
    "reward_distributions = [np.random.normal(loc, 1, 1000) for loc in range(1, k+1)]\n",
    "bandit = MultiArmedBandit(k, reward_distributions)\n",
    "\n",
    "# UCB Algorithm for Multi-Armed Bandit\n",
    "def ucb(bandit, N=1000, c=2):\n",
    "    Q = np.zeros(bandit.k)\n",
    "    N_a = np.zeros(bandit.k)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(1, N+1):\n",
    "        ucb_values = Q + c * np.sqrt(np.log(t) / (N_a + 1e-5))\n",
    "        arm = np.argmax(ucb_values)\n",
    "        reward = bandit.pull(arm)\n",
    "        N_a[arm] += 1\n",
    "        Q[arm] += (reward - Q[arm]) / N_a[arm]\n",
    "        total_reward += reward\n",
    "\n",
    "    return Q, total_reward\n",
    "\n",
    "Q_ucb, total_reward_ucb = ucb(bandit)\n",
    "print(\"Q-Values (UCB):\\n\", Q_ucb)\n",
    "print(\"Total Reward (UCB):\\n\", total_reward_ucb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_bandit(agent, bandit, N=1000):\n",
    "    total_reward = 0\n",
    "    for _ in range(N):\n",
    "        action = agent(bandit)\n",
    "        reward = bandit.pull(action)\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "# Example optimal strategy comparison\n",
    "def optimal_strategy(bandit):\n",
    "    return np.argmax(bandit.q_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward (Optimal Strategy):\n",
      " 9992.865927120662\n",
      "Total Reward (UCB):\n",
      " 9966.208897800314\n"
     ]
    }
   ],
   "source": [
    "total_reward_optimal = evaluate_bandit(optimal_strategy, bandit)\n",
    "print(\"Total Reward (Optimal Strategy):\\n\", total_reward_optimal)\n",
    "print(\"Total Reward (UCB):\\n\", total_reward_ucb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
